{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44573bcf",
   "metadata": {},
   "source": [
    "# 감성분석 -> 머신러닝\n",
    "- 데이터셋: 전처리\n",
    "- BoW 모델:\n",
    "    - 단어를 특성 벡터로 변환\n",
    "    - tf-idf 단어 적합성 평가\n",
    "    - 텍스트 데이터 정제\n",
    "    - 문서를 토큰으로 나누기\n",
    "- LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b741c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Story of a man who has unnatural feelings for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Airport '77 starts as a brand new luxury 747 p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This film lacked something I couldn't put my f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sorry everyone,,, I know this is supposed to b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When I was little my parents took me along to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  target\n",
       "0  Story of a man who has unnatural feelings for ...       0\n",
       "1  Airport '77 starts as a brand new luxury 747 p...       0\n",
       "2  This film lacked something I couldn't put my f...       0\n",
       "3  Sorry everyone,,, I know this is supposed to b...       0\n",
       "4  When I was little my parents took me along to ...       0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "file_lists = glob('C://python_src//pandas-data-analysis//machine_learning//data//train//neg//*.txt')\n",
    "pd_lists = []\n",
    "\n",
    "for file_path in file_lists[:500]:\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        data = {\n",
    "            'review' : f.read(),\n",
    "            'target' : 0\n",
    "        }\n",
    "\n",
    "    df = pd.DataFrame([data])\n",
    "    pd_lists.append(df)\n",
    "\n",
    "train_neg_df = pd.concat(pd_lists, ignore_index=True)\n",
    "train_neg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  target\n",
       "0  Bromwell High is a cartoon comedy. It ran at t...       1\n",
       "1  Homelessness (or Houselessness as George Carli...       1\n",
       "2  Brilliant over-acting by Lesley Ann Warren. Be...       1\n",
       "3  This is easily the most underrated film inn th...       1\n",
       "4  This is not the typical Mel Brooks film. It wa...       1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "file_lists = glob('C://python_src//pandas-data-analysis//machine_learning//data//train//pos//*.txt')\n",
    "pd_lists = []\n",
    "\n",
    "for file_path in file_lists[:500]:\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        data = {\n",
    "            'review' : f.read(),\n",
    "            'target' : 1\n",
    "        }\n",
    "\n",
    "    df = pd.DataFrame([data])\n",
    "    pd_lists.append(df)\n",
    "\n",
    "train_pos_df = pd.concat(pd_lists, ignore_index=True)\n",
    "train_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  target\n",
       "0  Bromwell High is a cartoon comedy. It ran at t...       1\n",
       "1  Homelessness (or Houselessness as George Carli...       1\n",
       "2  Brilliant over-acting by Lesley Ann Warren. Be...       1\n",
       "3  This is easily the most underrated film inn th...       1\n",
       "4  This is not the typical Mel Brooks film. It wa...       1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat( [train_pos_df, train_neg_df], ignore_index=True )\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"movie_data.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88267929",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('movie_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d82ea0a",
   "metadata": {},
   "source": [
    "## BOW(Bag of Words) 모델\n",
    "<!-- https://www.ibm.com/kr-ko/think/topics/bag-of-words -->\n",
    "- 문자를 숫자벡터 \n",
    "- 단어의 등장 횟수를 카운트\n",
    "---\n",
    "- 전체 훈련데이터에서 모든 고유한 단어(토큰)로 어휘 사전을 만들음\n",
    "- 각 문서(review data)를 사전을 기준으로 벡터화 -> n번째 단어가 문서에 3번 나오면, 벡터의 n번째 값이 3이 됨.\n",
    "> - 문서1: \"나는 영화가 좋다.\" <br>\n",
    "> - 문서2: \"나는 영화가 싫다.\" <br>\n",
    "> 사전: ('나는':0 '영화가':1 '좋다':2 '싫다':3) <br>\n",
    "- 벡터화는 사전의 크기만큼 모든 문장의 길이를 동일하게 함\n",
    "> - 문서 1벡터: 토큰 [0,1,2] <br> -> 벡터화[1,1,1,0]\n",
    "> - 문서 2벡터: 토큰 [0,1,3] <br> -> [1,1,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "    \"The sun is shining\",\n",
    "    \"The weather is sweet\"\n",
    "])\n",
    "\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db55c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a336a7",
   "metadata": {},
   "source": [
    "## tf(t,d)\n",
    "- 문서 d에 등장한 단어 t의 횟수를 tf(t,d)\n",
    "- Bow를 보완하면서 좀 더 정교한 텍스트 벡터화 방식 TF-IDF(Term Frequency - Inverse Document Frequency)\n",
    "    - TF: 특정 문서에서 자주 등장하는 단어\n",
    "    - IDF: 전체 문서에서 드물게 등장하는 단어\n",
    "- 특정 문서에서 자주 등장하지만, 전체 문장에서 드물게 등장하는 단어에 높은 가중치 부여 -> 그 문장을 잘 대표하는 핵심 단어를 찾는다\n",
    "- `TF(t,d)`: `단어 t가 문장 d에 나타난 횟수 / 문서 d의 모든 단어 수` \n",
    "- `IDF(t,D)`: `log( 총 문서 수[D] / 단어 t를 포함한 문서 수 df(t ) )` (why? _log_ 단어의 희귀성을 너무 과하게 반영하지 않도록 스케일링) \n",
    "- 분모에 +1(사이킷런의 경우): 분모가 0이 되는 것 방지\n",
    "- `log(1+[D]/1+df(f))`\n",
    "> `TF-IDF(t,d,D) = TF(t,d) x IDF(t,D)` \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- 예시 <br>\n",
    "        \"나는\"\n",
    "    - TF: 리뷰에 3번 나옴 (가중치 높음)\n",
    "    - IDF: 전체 10,000개 리뷰 중 9,000개 문장에서 나옴 (매우 낮음)\n",
    "    - TF-IDF: 높음 x 매우낮음 = 낮음(중요도가 낮다) <br>\n",
    "    <br>\n",
    "    \n",
    "    \"명작\"\n",
    "    - TF: 리뷰에 2번 나옴 (가중치 높음)\n",
    "    - IDF: 전체 10,000개 리뷰 중 50개 문장에서 나옴 (매우 높음)\n",
    "    - TF-IDF: 높음 x 매우높음 = 높음(핵심 단어)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87656259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정제... html tag 와 같은 불필요한 string이 보임... 특수기호 기타등등.  < - ' \n",
    "import re\n",
    "def preprocessor(s):\n",
    "    # 1. 영문, 공백, ., , 만 남기기\n",
    "    clean = re.sub(r'[^A-Za-z\\s.,]+', '', s)\n",
    "    # 2. 연속된 마침표(...)를 마침표 하나로\n",
    "    clean = re.sub(r'\\.{2,}', '.', clean)\n",
    "    # 3. 연속된 공백 정리\n",
    "    clean = re.sub(r'\\s+', ' ', clean).strip()\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0a86d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df.review.apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f883dd95",
   "metadata": {},
   "source": [
    "### 불용어(Stopwords)\n",
    "- 자연어 처리 시 분석에 크게 기여하지 않는, 의미가 거의 없거나 중요하지 않은 단어\n",
    "-  'the', 'a', 'an', 'is', 'in', 'and' ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4121eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords') #불용어 사전 다운로드\n",
    "stops = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed7e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() #전처리 완료, 정규식을 이용한??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce14a3e",
   "metadata": {},
   "source": [
    "### 어간추출 (Stemming)\n",
    "- 단어의 접사(suffix, prefix)를 잘라서 원형(어간)으로 통일하는 과정\n",
    "- PorterStemmer, LancasterStemmer 등 규칙 기반 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a693731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "# 어간 추출 Stemming: 단어의 접미사(-s,-es,-ing ...)를 강제로 제거 -> '단어의 원형' 찾기\n",
    "tokenizer(df.review[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36be1b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer('runners like running')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27abffa0",
   "metadata": {},
   "source": [
    "##### 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.review\n",
    "y = df.target\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff9284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간추출 -> stopwords에 포함된 단어 제거\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split() if word not in stops]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f776b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    tokenizer=tokenizer_porter,\n",
    "    ngram_range=(1,1) #(1,1) 유니그램(unigram, 단일단어)만 사용)\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('clf', LogisticRegression())\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cb9587",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c97db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터 로드\n",
    "\n",
    "# 토크나이저 함수 정의\n",
    "    # 텍스트 전처리\n",
    "    # 공백을 기준으로, 단어단위로 분리\n",
    "    # 영어는 전부 소문자로 변환\n",
    "    # 어간 추출\n",
    "    # 불용어 제거\n",
    "\n",
    "# TFIDF를 정의\n",
    "    # 토크나이저 매개변수 = 토크나이저 함수\n",
    "    # ngram=(1,1)\n",
    "\n",
    "# 파이프라인으로 tfidf, 머신러닝\n",
    "\n",
    "# 파이프라인으로 학습\n",
    "\n",
    "# 파이프라인으로 평가(calssification_report)\n",
    "\n",
    "# 과적합 여부 확인\n",
    "    # train data와 test data로 성능 비교\n",
    "\n",
    "# train 폴더의 데이터로 학습 후, test폴더의 데이터로 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51ea2470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  target\n",
       "0  I went and saw this movie last night after bei...       1\n",
       "1  Actor turned director Bill Paxton follows up h...       1\n",
       "2  As a recreational golfer with some knowledge o...       1\n",
       "3  I saw this film in a sneak preview, and it is ...       1\n",
       "4  Bill Paxton has taken the true story of the 19...       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "file_lists = glob('C://python_src//pandas-data-analysis//machine_learning//data//test//neg//*.txt')\n",
    "pd_lists = []\n",
    "\n",
    "for file_path in file_lists[:500]:\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        data = {\n",
    "            'review' : f.read(),\n",
    "            'target' : 0\n",
    "        }\n",
    "\n",
    "    train_df = pd.DataFrame([data])\n",
    "    pd_lists.append(train_df)\n",
    "\n",
    "test_neg_df = pd.concat(pd_lists, ignore_index=True)\n",
    "\n",
    "file_lists = glob('C://python_src//pandas-data-analysis//machine_learning//data//test//pos//*.txt')\n",
    "pd_lists = []\n",
    "\n",
    "for file_path in file_lists[:500]:\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        data = {\n",
    "            'review' : f.read(),\n",
    "            'target' : 1\n",
    "        }\n",
    "\n",
    "    test_df = pd.DataFrame([data])\n",
    "    pd_lists.append(test_df)\n",
    "\n",
    "test_pos_df = pd.concat(pd_lists, ignore_index=True)\n",
    "test_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4771aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat( [train_pos_df, train_neg_df], ignore_index=True )\n",
    "test_df = pd.concat( [test_pos_df, test_neg_df], ignore_index=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b9123e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  target\n",
       "0  Bromwell High is a cartoon comedy. It ran at t...       1\n",
       "1  Homelessness (or Houselessness as George Carli...       1\n",
       "2  Brilliant over-acting by Lesley Ann Warren. Be...       1\n",
       "3  This is easily the most underrated film inn th...       1\n",
       "4  This is not the typical Mel Brooks film. It wa...       1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bdbcf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    # 전처리\n",
    "    # 1. 영문, 공백, ., , 만 남기기\n",
    "    clean = re.sub(r'[^A-Za-z\\s.,]+', '', text)\n",
    "    # 2. 연속된 마침표(...)를 마침표 하나로\n",
    "    clean = re.sub(r'\\.{2,}', '.', clean)\n",
    "    # 3. 연속된 공백 정리\n",
    "    clean = re.sub(r'\\s+', ' ', clean).strip()\n",
    "    # 단어분리-어간분리-불용어제거\n",
    "    return [porter.stem(word) for word in clean.split() if word not in stops]\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    tokenizer=custom_tokenizer,\n",
    "    ngram_range=(1,1),\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('tfid',tfidf),\n",
    "    ('clf',LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80a9498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = train_df.sample(200, random_state=42)\n",
    "test_sample = test_df.sample(200, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6504877",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stops' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m X_test  = test_sample[\u001b[33m'\u001b[39m\u001b[33mreview\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      8\u001b[39m y_test  = test_sample[\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m y_pred = pipe.predict(X_test)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred, zero_division=\u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python_src\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python_src\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:655\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    649\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    650\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    651\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    652\u001b[39m     )\n\u001b[32m    654\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._check_method_params(method=\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, props=params)\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m Xt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps) - \u001b[32m1\u001b[39m)):\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python_src\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:589\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, X, y, routed_params, raw_params)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[32m    583\u001b[39m step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    584\u001b[39m     step_idx=step_idx,\n\u001b[32m    585\u001b[39m     step_params=routed_params[name],\n\u001b[32m    586\u001b[39m     all_params=raw_params,\n\u001b[32m    587\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m X, fitted_transformer = \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPipeline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[32m    600\u001b[39m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[32m    601\u001b[39m \u001b[38;5;28mself\u001b[39m.steps[step_idx] = (name, fitted_transformer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python_src\\.venv\\Lib\\site-packages\\joblib\\memory.py:326\u001b[39m, in \u001b[36mNotMemorizedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python_src\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:1540\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1539\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1541\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1542\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1543\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1544\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python_src\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2105\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2099\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2100\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2101\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2102\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2103\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2104\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2105\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2106\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2108\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python_src\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python_src\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1377\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1369\u001b[39m             warnings.warn(\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1374\u001b[39m             )\n\u001b[32m   1375\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1377\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1380\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python_src\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1264\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[32m   1263\u001b[39m     feature_counter = {}\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1265\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1266\u001b[39m             feature_idx = vocabulary[feature]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python_src\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:106\u001b[39m, in \u001b[36m_analyze\u001b[39m\u001b[34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[39m\n\u001b[32m    104\u001b[39m     doc = preprocessor(doc)\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     doc = \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mcustom_tokenizer\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     17\u001b[39m clean = re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m, clean).strip()\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 단어분리-어간분리-불용어제거\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [porter.stem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m clean.split() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstops\u001b[49m]\n",
      "\u001b[31mNameError\u001b[39m: name 'stops' is not defined"
     ]
    }
   ],
   "source": [
    "# 샘플로 테스트\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train = train_sample['review']\n",
    "y_train = train_sample['target']\n",
    "X_test  = test_sample['review']\n",
    "y_test  = test_sample['target']\n",
    "\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12904e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터\n",
    "\n",
    "# X_train = train_df['review']\n",
    "# X_test  = test_df['target']\n",
    "# y_train = train_df['review']\n",
    "# y_test  = test_df['target']\n",
    "\n",
    "# pipe.fit(X_train, y_train)\n",
    "# y_pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7cdc5c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m cm = confusion_matrix(\u001b[43my_test\u001b[49m, y_pred)\n\u001b[32m      7\u001b[39m cm_df = pd.DataFrame(cm, index=pipe.classes_, columns=pipe.classes_)\n\u001b[32m      9\u001b[39m plt.figure(figsize=(\u001b[32m5\u001b[39m,\u001b[32m4\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=pipe.classes_, columns=pipe.classes_)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Sample 200)\")\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b157c991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
